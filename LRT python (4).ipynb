{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b56b217d",
   "metadata": {},
   "source": [
    "# Likelihood Ratio Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fde879",
   "metadata": {},
   "source": [
    "## Normal model with variance known\n",
    "            \n",
    "Suppose that we have a sample $x_{1},..., x_{n}$ and we assume the following model\n",
    "\n",
    "$$\n",
    " \\bigg\\{  N\\big( \\mu, \\sigma^{2}    \\big) ; \\mu \\in \\mathbb{R} \\bigg\\} \n",
    "$$\n",
    "\n",
    "That is a normal distribution with mean $\\mu$, unknown and variance $\\sigma^{2}$, known. We want to test\n",
    "\n",
    "$$\n",
    "H_{0} : \\mu = \\mu_{0} \\ \\text{against}  \\  H_{1} : \\mu \\neq \\mu_{0}\n",
    "$$\n",
    "\n",
    "The idea is to set up a test of hypothesis based on the Maximum Likelihood Estimator (MLE) $\\hat{\\mu}$.\n",
    "\n",
    "## Likelihood Ratio Test (LRT)\n",
    "\n",
    "By definition, the Likelihood Ratio Test (LRT) statistic is given by\n",
    "\n",
    "\n",
    "$$\n",
    " \\Lambda(x_{1},..., x_{n})  = \\frac{ \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}    e^{-\\frac{1}{2} \\big( \\frac{x_{i} - \\hat{\\mu}}{\\sigma} \\big)^2}}{\\prod_{i=1}^{n}  \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}  \\ e^{   -\\frac{1}{2}    \\big( \\frac{x_{i} - \\mu_{0}} {\\sigma} \\big)^2    }    }\n",
    "$$\n",
    "\n",
    "Which simplifies to  \n",
    "\n",
    "$$\n",
    " \\Lambda(x_{1},..., x_{n})  = exp \\bigg(      -\\frac{1}{2 \\sigma^{2}}  \\sum_{i=1}^{n} \\big(x_{i} -  \\hat{\\mu}\\big)^{2}  +   \\frac{1}{2 \\sigma^{2}}  \\sum_{i=1}^{n} \\big(x_{i} -  \\mu_{0} \\big)^{2}                \\bigg)\n",
    "$$\n",
    "\n",
    "Where $\\hat{\\mu}$ is the Maximum Likelihood estimator for $\\mu$ and is equal to $\\bar{x}$.\n",
    "\n",
    "Now, it is interesting to note that twice the logarithm of  $ \\Lambda(x_{1},..., x_{n})$ gives us a convenient expression (see next slide)\n",
    "\n",
    "\n",
    "## Likelihood Ratio Test (LRT) \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "2 ln \\big( \\Lambda(x_{1},..., x_{n})  \\big) & = -\\frac{1}{ \\sigma^{2}}  \\sum_{i=1}^{n} \\big(x_{i} - \\bar{x}\\big)^{2}  +   \\frac{1}{ \\sigma^{2}}  \\sum_{i=1}^{n} \\big(x_{i}^{2}  -  \\mu_{0} \\big)^{2}     \\\\  \n",
    "& =  -\\frac{1}{ \\sigma^{2}}  \\sum_{i=1}^{n} \\big(x_{i}^{2} - 2 x_{i}  \\bar{x} +  \\bar{x}^{2}   \\big)  +   \\frac{1}{ \\sigma^{2}}  \\sum_{i=1}^{n} \\big(x_{i}^{2}  -2 x_{i}  \\mu_{0} + \\mu_{0}^{2} \\big)     \\\\  \n",
    "& =  -\\frac{1}{ \\sigma^{2}}            \\bigg(    \\sum_{i=1}^{n} x_{i}^{2}  - 2 \\bar{x}  \\sum_{i=1}^{n} x_{i}   +  \\sum_{i=1}^{n}  \\bar{x}^{2}   \\bigg)  +   \\frac{1}{ \\sigma^{2}}                                                                                     \\bigg(   \\sum_{i=1}^{n}  x_{i}^{2}  -2 \\mu_{0}  \\sum_{i=1}^{n}  x_{i}^{2}    +  \\sum_{i=1}^{n}  \\mu_{0}^{2} \\bigg)     \\\\  \n",
    "& =  \\frac{1}{ \\sigma^{2}}            \\bigg(   - \\sum_{i=1}^{n} x_{i}^{2}  + 2 \\bar{x}  \\sum_{i=1}^{n} x_{i}   -  \\sum_{i=1}^{n} \\bar{x}^{2}   +  \\sum_{i=1}^{n}  x_{i}^{2}  -2 \\mu_{0}  \\sum_{i=1}^{n}  x_{i}^{2}    +  \\sum_{i=1}^{n}  \\mu_{0}^{2} \\bigg)     \\\\  \n",
    "& =  \\frac{n}{ \\sigma^{2}}            \\bigg(   -    \\frac{1}{n}  \\sum_{i=1}^{n} x_{i}^{2}  + 2 \\bar{x}  \\frac{1}{n} \\sum_{i=1}^{n} x_{i}   -   \\frac{1}{n} n \\bar{x}^{2}   +  \\frac{1}{n} \\sum_{i=1}^{n}  x_{i}^{2}  -2 \\mu_{0}  \\frac{1}{n} \\sum_{i=1}^{n}  x_{i}   +  \\frac{1}{n} n \\mu_{0}^{2} \\bigg)     \\\\  \n",
    "& =  \\frac{n}{ \\sigma^{2}}            \\bigg(    2 \\bar{x}^{2}  -    \\bar{x}^{2}   -2 \\mu_{0} \\bar{x}  +  \\mu_{0}^{2} \\bigg)     \\\\  \n",
    "& =  \\frac{n}{ \\sigma^{2}}            \\bigg(    \\bar{x}    -2 \\mu_{0} \\bar{x}  +  \\mu_{0}^{2} \\bigg)     = n       \\bigg(  \\frac{   \\bar{x}    -  \\mu_{0} }{\\sigma} \\bigg)^{2}     \\\\  \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## Likelihood Ratio Test (LRT)\n",
    "\n",
    "And by the CLT, \n",
    "we know that $ \\sqrt{ n}       \\big(  \\frac{   \\bar{x}    -  \\mu_{0} }{\\sigma} \\big) \\sim N \\big(0, 1 \\big)$.\n",
    "\n",
    "Since, by definition, \n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} Z_{i}^{2}  \\sim \\chi_{n}^{2}\n",
    "$$\n",
    "\n",
    "It follows that \n",
    "\n",
    "$$\n",
    "2 ln \\big( \\Lambda(x_{1},..., x_{n})  \\big) \\sim \\chi_{1}^{2}\n",
    "$$\n",
    "\n",
    "And therefore $H_{0}$ is rejected if $n       \\bigg(  \\frac{   \\bar{x}    -  \\mu_{0} }{\\sigma} \\bigg)^{2}   > \\chi_{1, 1 - \\alpha}^{2} $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b275991",
   "metadata": {},
   "source": [
    "## Working example 1\n",
    "\n",
    "Suppost that we get the following sample of size $40$:\n",
    "\n",
    " [1] 11.75  9.05  6.37 11.44 10.10 15.27  9.26 15.00 10.80 10.60 12.98 10.76 \n",
    "     13.69 13.99 10.19 14.10 13.79 13.36 14.69 13.72 10.77 11.12 15.66\n",
    "     12.73 10.66  6.46 10.11  9.42 16.54 20.21 11.18 15.83  9.57 11.87 10.08\n",
    "     10.68 13.52 15.39 14.93 11.63\n",
    "\n",
    "The data are available here:\n",
    "\n",
    "https://github.com/JRigh/Likelihood-Ratio-Tests/blob/main/data.csv\n",
    "\n",
    "For what integer value(s) of $\\mu_{0}$ the LRT statistic \n",
    "is NOT rejected at a significance level of $5 \\%$? In other words, what are acceptable population mean for that dataset? Answer: $12$ and $13$ (see code below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a048fe19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['5', '232.43648027777778', 'Yes'],\n",
       "       ['6', '172.5987025', 'Yes'],\n",
       "       ['7', '121.6498136111111', 'Yes'],\n",
       "       ['8', '79.58981361111111', 'Yes'],\n",
       "       ['9', '46.4187025', 'Yes'],\n",
       "       ['10', '22.136480277777775', 'Yes'],\n",
       "       ['11', '6.743146944444444', 'Yes'],\n",
       "       ['12', '0.2387024999999998', 'No'],\n",
       "       ['13', '2.623146944444445', 'No'],\n",
       "       ['14', '13.89648027777778', 'Yes'],\n",
       "       ['15', '34.0587025', 'Yes']], dtype='<U32')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2\n",
    "\n",
    "\n",
    "# import the data\n",
    "data = pd.read_csv(\"C:/Users/julia/OneDrive/Desktop/github/36. LRT/data.csv\")\n",
    "data\n",
    "\n",
    "# Calculate LRT statistic for each mu0\n",
    "lrt_statistic = n * ((np.mean(data['x']) - mu0)**2 / sigma**2)\n",
    "\n",
    "# Calculate the critical value from the chi-squared distribution\n",
    "critical_value = chi2.ppf(1 - alpha, df=1)\n",
    "\n",
    "# Perform the Likelihood Ratio Test and make decisions\n",
    "decisions = np.where(lrt_statistic > critical_value, \"Yes\", \"No\")\n",
    "\n",
    "# Create a results data array\n",
    "results = np.column_stack((mu0, lrt_statistic, decisions))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10317f6",
   "metadata": {},
   "source": [
    "## Univariate Gaussian Mixtures: introduction\n",
    "\n",
    "Mixtures are complicated distributions build from simpler ones. In this respect, these distributions can be viewed as a weighted combination of densities. Let $Y$ be a random variable (or a $d$-dimensional random vector in the multivariate case) and $y$ be any observed values of this random variable. Then $Y$ obeys a finite mixture distribution if its density can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "f(y) =  \\lambda_{1}f_{1}( y )+ ... +  \\lambda_{k}f_{k}( y ) = \\sum_{j=1}^{k} \\lambda_{j}f_{j}( y ) \\ ,\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "provided that $\\lambda_{j} > 0$ and $\\sum_{j=1}^{k} \\lambda_{j} = 1$.\n",
    "\n",
    "The weights $\\lambda_{j}$ are called the \\textit{mixing proportions} and $f_{j}( y )$ are called the \\textit{component densities}.\n",
    "\n",
    "\n",
    "Further, a $k$-component parametric finite mixture model has the form:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "f(y\\mid \\boldsymbol{\\Psi}) = \\sum_{j=1}^{k} \\lambda_{j}f_{j}( y \\mid \\boldsymbol{\\theta}_{j}) \\ .\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "In the case of a two-component mixture, that is with $k=2$, we have the mixture parameter vector is $\\boldsymbol{\\Psi}= (\\lambda_{1},  \\lambda_{2}, \\mu_{1},  \\mu_{2},  \\sigma_{1}^{2},  \\sigma_{2}^{2})$; the number of components is $k$; the component density parameters are $\\boldsymbol{\\theta_{1}}= (\\mu_{1}, \\sigma_{1}^{2})$ and $\\boldsymbol{\\theta_{2}}= (\\mu_{2}, \\sigma_{2}^{2})$; the mixing proportions are  $\\lambda_{1}$ and $\\lambda_{2} =  ( 1 -  \\lambda_{1})$. \n",
    "\n",
    "\n",
    "## Detection of the number of components $k$ in a univariate Gaussian mixture using the likelihood ratio test\n",
    "\n",
    "Among the many methods for determining the number of components in a mixture model, the Likelihood Ratio Test (LRT) is one commonly used in practice. A good explanation is given in McLachlan and Peel (2000). Roughly, the procedure proposes to test sequentially for a null hypothesis of a model with the smallest number of component $k$ in the mixture against an alternative hypothesis of $k+1$ components, that is:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "H_{0}: k = k_{0}   \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{against} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ H_{1}: k = k_{0}+1\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The test statistic considered is:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "-2 log( \\Lambda ) = 2 \\big( logL(\\hat{\\boldsymbol{\\Psi}}_{1}) - logL(\\hat{\\boldsymbol{\\Psi}}_{0}) \\big) \\ ,\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "with $\\hat{\\boldsymbol{\\Psi}}_{1}$ and $\\hat{\\boldsymbol{\\Psi}}_{0}$ being the maximum likelihood estimates for the model with the largest respectively the smallest number of components.\n",
    "\n",
    "\n",
    "A small value for $\\Lambda$ or similarly a large value for $-2log(\\Lambda)$ will indicate strong evidence against the null hypothesis.\n",
    "The approximate distribution of the statistic under $H_{0}$ is not clearly identified, that is why we often resort to bootstrap in practice to take a decision with regards to the null hypothesis. A parametric bootstrap for this statistic is implemented in the mixtools package. This procedure is practically useful because it can be carried prior an EM or a bayesian analysis to have a guess about $k$, unlike information critera, introduced in the next section, that are used conditionally to a preliminary parameter estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85295b13",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Bijma, F., Jonker M., Van der Vaart, A., An Introduction to Mathematical Statistics. Amsterdam University Press., 2016\n",
    "\n",
    "R.V.Hogg and E.A.Tanis: Probability and Statistical Inference, Sixth Edition, Prentice Hall, Upper Saddle River, N.J., 2001.\n",
    "\n",
    "McLachlan, G. and Peel, D., Finite mixture models. 0471006262, John Wiley \\& Sons., 2000\n",
    "\n",
    "The R Project for Statistical Computing:\n",
    "https://www.r-project.org/\n",
    "\n",
    "Python:\n",
    "https://www.python.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835c90e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
